## **ğŸ§© Submodule 2.1.1 Who Owns the Loop?**

### ğŸŸ¢ Mental Model (core idea)

A **loop** is simply:

> â€œDo something â†’ check â†’ repeatâ€

The _only_ question that matters is:

> **Who is allowed to decide whether the loop continues?**

There are only **two possible answers**.

---

### âŒ Case 1: The AI owns the loop (BAD)

Example in plain English:

> â€œKeep improving the answer until you think itâ€™s good.â€

Problems:

- â€œGoodâ€ is undefined
    
- Confidence â‰  correctness
    
- No upper bound
    
- No accountability
    

This is how agents:

- loop forever
    
- hallucinate confidence
    
- burn time/money
    

---

### âœ… Case 2: The SYSTEM owns the loop (GOOD)

Example:

> â€œImprove the answer **at most 3 times**.  
> If still unsatisfactory, stop and escalate.â€

Here:

- The AI participates **inside** the loop
    
- The system controls **existence of the loop**
    

This single shift is the difference between:

- a demo
    
- a production system
    

---

## ğŸ”µ Why this concept exists (real failure reason)

Early â€œagentâ€ systems failed not because:

- LLMs were weak âŒ
    
- prompts were bad âŒ
    

They failed because:

> **No one was in charge of repetition.**

Humans assume:

- iteration magically converges  
    Engineering reality:
    
- iteration often oscillates or diverges
    

---

## ğŸŸ£ The 3 Non-Negotiable Parts of a Controlled Loop

Every safe loop must have **all three**:

### 1ï¸âƒ£ A **Counter**

- Max attempts
    
- Max revisions
    
- Max tool calls
    

Written as:

> â€œNo more than N timesâ€

---

### 2ï¸âƒ£ A **Progress Check**

Not â€œfeels betterâ€.

But something like:

- shorter than before
    
- fewer errors than before
    
- passed a checklist
    
- human approved
    

If you cannot define progress:  
ğŸ‘‰ **you do not have a loop, you have hope**

---

### 3ï¸âƒ£ A **Hard Stop**

A state where:

- the system **must exit**
    
- even if the task is unfinished
    

Examples:

- escalate to human
    
- return partial result
    
- fail safely
    

The AI is **never allowed** to override this.

---

## ğŸ§ª Simple Paper Example (no tech)

Imagine designing this on paper:

`[Start]    â†“ [Draft]    â†“ [Evaluate]    â”œâ”€ acceptable â†’ [Finish]    â””â”€ not acceptable â†’ [Revise]                         â†“                  (increment counter)                         â†“                  counter < 3 ?                    â”œ yes â†’ back to [Draft]                    â”” no  â†’ [Escalate to Human]`

That diagram alone already contains:

- control
    
- safety
    
- autonomy (bounded)
    

No code needed.

---

## ğŸ› ï¸ Common Mistakes (important)

âŒ â€œUntil itâ€™s good enoughâ€  
âŒ â€œUntil the AI is confidentâ€  
âŒ â€œLet the agent decideâ€

All of these are **control failures**, not intelligence problems.

---

## ğŸš§ MINI PROJECT (MANDATORY, PAPER ONLY)

### ğŸ”¨ Task: **Design a Controlled Loop**

Pick **any one task**:

- writing
    
- researching
    
- reviewing
    
- planning
    

On paper, define **explicitly**:

1. What step repeats?
    
2. What never changes?
    
3. What is counted?
    
4. What proves progress?
    
5. What happens when the limit is hit?
    

You donâ€™t need to show me the drawing.  
You just need to **be able to explain it**.

### ğŸ§© **Submodule 2.1.2: Exit Conditions**

> **Unit 2.1.1 answered:** _Who owns repetition?_  
> **Unit 2.1.2 answers:** _Why and when must the system stop?_

This is where most agent systems quietly fail.

---

## ğŸŸ¢ Mental Model â€” _What an exit condition really is_

An **exit condition** is **not**:

- â€œLooks goodâ€
    
- â€œThe model is confidentâ€
    
- â€œNo more ideasâ€
    

An exit condition is:

> **An externally verifiable fact that ends the process.**

If a system cannot point to a _fact_ and say

> â€œBecause this is true, we must stopâ€  
> then it doesnâ€™t have an exit condition.

It has a _feeling_.

---

## ğŸ”µ Why this exists (real failure)

Most runaway agents fail **even with counters** because:

- They stop for the **wrong reason**
    
- Or they never stop because the reason is subjective
    

Examples of bad exits:

- â€œAnswer seems completeâ€
    
- â€œQuality improvedâ€
    
- â€œNo obvious errorsâ€
    

These cannot be enforced by code or policy.

---

## ğŸŸ£ The Three Legitimate Exit Types

Every correct exit condition belongs to **one of these three**.

---

### 1ï¸âƒ£ **Goal-Satisfied Exit**

> The original goal is demonstrably met.

Examples:

- All checklist items are satisfied
    
- Output passes validation rules
    
- Human explicitly approves
    

Key property:

- Binary (yes/no)
    
- Independent of model opinion
    

---

### 2ï¸âƒ£ **Budget-Exhausted Exit**

> The system is no longer allowed to continue.

Examples:

- `cnt >= max`
    
- time limit reached
    
- cost limit reached
    

Key property:

- Mechanical
    
- Non-negotiable
    

This is the most important safety exit.

---

### 3ï¸âƒ£ **Escalation Exit**

> The system admits it cannot proceed autonomously.

Examples:

- Human review required
    
- Partial result returned
    
- Task handed off
    

This is not failure.  
This is **designed humility**.

---

## ğŸš« What is NOT an exit condition (critical)

âŒ â€œThe agent feels doneâ€  
âŒ â€œNo more tools to tryâ€  
âŒ â€œThe response is good enoughâ€

These are **internal states**, not exits.

---

## ğŸ§ª Paper Example (tight and clean)

`[Evaluate]`
   `â†“`
`Is output valid?`
   `â”œâ”€ yes â†’ [SAVE & END]`
   `â””â”€ no`
        `â†“`
`Is cnt >= max?`
   `â”œâ”€ yes â†’ [ESCALATE]`
   `â””â”€ no  â†’ [REFINE]`

Notice:
- Evaluation does **not** decide looping
    
- Control logic does

## ğŸ› ï¸ Common Exit Bugs (watch for these)

### âŒ Exit-by-Confidence

- Agent says â€œthis is finalâ€
    
- System believes it
    

### âŒ Exit-by-Exhaustion Only

- System always runs until max
    
- Never recognizes success early
    

### âŒ Exit-by-Silence

- Nothing explicitly ends
    
- Loop falls out accidentally
    

---

## ğŸš§ MINI PROJECT (PAPER ONLY)

### ğŸ”¨ Task: **Define Real Exit Conditions**

Using **your existing diagram**, do this:

1. Circle **every place** where the system can stop
    
2. Label each stop as:
    
    - Goal-satisfied
        
    - Budget-exhausted
        
    - Escalation
        
3. Remove any stop that depends on _judgment words_  
    (good, better, enough, confident)
    

If you canâ€™t label a stop cleanly â†’ itâ€™s invalid.

---

## ğŸ”’ One invariant to remember

> **Exit conditions must be enforceable by the system, not argued by the agent.**

Write that in your notebook. Itâ€™s foundational.

## ğŸ§© **Submodule 2.1.3: Human-in-the-Loop (Done Right)**

> Up to now, you learned **how systems loop and stop**.  
> Now we answer: **when (and how) humans are allowed to intervene**.

Most systems get this wrong.

---

## ğŸŸ¢ Mental Model â€” what HITL really is

**Human-in-the-Loop is NOT:**

- â€œAsk a human whenever confusedâ€
    
- â€œLet the human fix thingsâ€
    
- â€œPause and wait for approval everywhereâ€
    

**Human-in-the-Loop IS:**

> A **controlled escalation mechanism** triggered by the system  
> when autonomy is no longer justified.

Humans are **not part of the loop**.  
They are **above the loop**.

---

## ğŸ”µ Why this concept exists (real failures)

Systems without HITL:

- hallucinate confidently
    
- fail silently
    
- cause damage at scale
    

Systems with _bad_ HITL:

- stop too often
    
- depend on humans for trivial decisions
    
- lose all automation value
    

The goal is **precision**, not safety theater.

---

## ğŸŸ£ The Three Legitimate Reasons to Involve a Human

A human may be invoked **only** for one of these reasons.

---

### 1ï¸âƒ£ **Authority Transfer**

> The system is not allowed to decide this.

Examples:

- Publishing content
    
- Approving irreversible actions
    
- Legal / ethical boundaries
    

Here:

- Human is the **final authority**
    
- The system must stop and wait
    

---

### 2ï¸âƒ£ **Ambiguity Resolution**

> Multiple valid paths exist, and preference matters.

Examples:

- Tone choices
    
- Trade-offs
    
- Subjective judgments
    

Here:

- Human provides _direction_
    
- System continues execution afterward
    

---

### 3ï¸âƒ£ **Failure Escalation**

> The system has exhausted safe autonomy.

Examples:

- Budget exhausted
    
- Repeated failure
    
- Conflicting signals
    

Here:

- Human helps **re-frame**
    
- System decides whether to continue
    

This is the case you designed â€” correctly.

---

## ğŸš« What humans must NEVER do

This is critical.

Humans must NOT:

- Reset counters directly
    
- Override stop conditions
    
- Secretly â€œjust allow one more loopâ€
    
- Modify control state ad-hoc
    

If they do, the system becomes:

- non-deterministic
    
- un-auditable
    
- impossible to debug
    

Humans advise.  
Systems decide.

---

## ğŸ§ª Clean HITL Pattern (paper form)

Here is the **canonical structure**:

`[System detects boundary]`
        `â†“`
`[Escalate to Human]`
        `â†“`
`[Human provides input]`
        `â†“`
`[System applies policy]`
        `â†“`
`[Continue OR Terminate]`

Notice:
- Human never jumps back into the loop
    
- Control always returns to the system
    

---

## ğŸ› ï¸ Common HITL Anti-Patterns (watch out)

### âŒ â€œApproval Everywhereâ€

- Humans approve every step
    
- No scalability
    

### âŒ â€œHuman as Fix-It Buttonâ€

- System offloads responsibility
    
- No learning, no structure
    

### âŒ â€œSilent Overrideâ€

- Human secretly edits state
    
- Violates invariants
    

You avoided these â€” thatâ€™s good.

---

## ğŸ”’ One non-negotiable invariant

> **Human input is data, not control.**

If you remember only one line from this submodule, make it this.

---

## ğŸš§ MINI PROJECT (MENTAL ONLY)

Using **your existing diagram**, answer:

1. Why is the human invoked _here_ and not earlier?
    
2. What exact question is the system asking the human?
    
3. What would happen if the human didnâ€™t respond?
    
4. Could this HITL be removed safely in the future?
    

If you can answer these, your HITL design is correct.