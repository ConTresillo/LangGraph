## ðŸ“¦ Module 2.3 â€” **Tool Execution Boundaries**

> This module answers one question:  
> **Who is allowed to act on the world?**

Most agent bugs live here.

## ðŸ§© Submodule 2.3.1

# **LLMs Suggest. Systems Execute.**

This is a **hard boundary**, not a guideline.

---

## ðŸŸ¢ Mental Model

An LLM is **not an actor**.  
It is a **proposer**.

It can:

- reason
    
- plan
    
- suggest
    
- describe actions
    

It must **never**:

- execute actions
    
- mutate control state
    
- call tools directly
    
- decide side effects
    

All real-world effects happen **outside** the LLM.

---

## ðŸ”µ Why this boundary exists (real failures)

When LLMs are allowed to:

- execute tools directly
    
- write files freely
    
- call APIs autonomously
    

You get:

- repeated destructive actions
    
- infinite tool loops
    
- irreversible side effects
    
- security disasters
    

This is why ReAct was designed the way it was.

---

## ðŸŸ£ The Correct Execution Pattern (conceptual)

Every tool interaction must follow this shape:

```
LLM â†’ proposes action + arguments
SYSTEM â†’ validates proposal
SYSTEM â†’ executes tool
SYSTEM â†’ observes result
SYSTEM â†’ updates state
```
The LLM is **never in the middle** of this chain.

---

## ðŸš« What the LLM must NEVER do

âŒ â€œI will now save the fileâ€  
âŒ â€œCalling the APIâ€¦â€  
âŒ â€œExecuting searchâ€¦â€

These are **lies** unless the system confirms them.

The LLM should only say:

> â€œI propose calling X with Y.â€

---

## ðŸ§ª Example (plain English)

### âŒ Bad

> â€œI saved the file successfully.â€

No it didnâ€™t.

---

### âœ… Good

> â€œThe next step would be to save the file at location X.â€

Then the **system** decides whether that happens.

---

## ðŸ”’ One critical invariant

> **The LLM may describe intent.  
> Only the system may cause effects.**

If you violate this:

- control collapses
    
- safety collapses
    
- debugging becomes impossible
    

---

## ðŸ› ï¸ How this fits YOUR design

In your diagram:

- `make draft` â†’ LLM work (pure computation)
    
- `evaluate` â†’ LLM or rule-based judgment
    
- `apply refinements` â†’ LLM proposes edits
    
- `save` â†’ **SYSTEM action**
    
- `reset cntr` â†’ **SYSTEM action**
    
- `check cntr/max` â†’ **SYSTEM action**
    

Notice:

- LLM never touches `cnt`
    
- LLM never decides save
    
- LLM never decides escalation
    

You already did this correctly â€” intuitively.

---

## ðŸ§ª Subtle but important distinction

There are **two kinds of â€œtoolsâ€**:

### 1ï¸âƒ£ **Pure tools** (safe)

- text transform
    
- summarization
    
- rewriting
    
- classification
    

These can be LLM-internal.

---

### 2ï¸âƒ£ **Effectful tools** (dangerous)

- file I/O
    
- APIs
    
- network
    
- databases
    
- payments
    
- emails
    

These must always be:

- proposed by LLM
    
- executed by system
    

Never merged.

---

## ðŸš§ MINI PROJECT (MENTAL ONLY)

Take your system and answer:

1. Which steps are **pure computation**?
    
2. Which steps cause **external effects**?
    
3. Which steps must be **system-owned** no matter what?
    
4. What would break if the LLM executed tools directly?
    

If you can answer this, you fully understand Module 2.3.

---

## ðŸ”‘ One-sentence lock-in

> **Autonomy is allowed in reasoning,  
> never in execution.**

Write that down. Itâ€™s non-negotiable.


